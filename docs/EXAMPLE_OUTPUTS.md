# EXAMPLE OUTPUTS - Feature & Model Analysis

## üìä Example 1: Feature Importance Analysis

### Command
```
Dashboard ‚Üí Feature Importance ‚Üí XGBoost Method ‚Üí Calculate
```

### Console Output
```
üìä Calculating feature importance using xgboost...
‚úÖ Calculated importance for 127 features

üèÜ Top 10 Features (XGBOOST Method):
   1. home_elo_rating                       : 0.124567
   2. away_elo_rating                       : 0.118432
   3. home_rest_days                        : 0.089234
   4. home_win_pct_last_10                  : 0.076891
   5. away_rest_days                        : 0.072456
   6. home_off_rating_last_5                : 0.065234
   7. away_def_rating_last_5                : 0.058923
   8. home_court_advantage                  : 0.051234
   9. home_pace_last_10                     : 0.047891
  10. away_win_pct_last_10                  : 0.044567

üíæ Plot saved to: analysis_results/feature_importance_xgboost.png
```

### Interpretation
- **ELO ratings** are most important (12-13% importance)
- **Rest days** significantly impact predictions (7-9%)
- **Recent form** (win % last 10) matters (4-8%)
- Features beyond top 20 likely contribute <2% each

### Action Items
- ‚úÖ Keep all top 10 features
- ‚ö†Ô∏è Test if top 15-20 perform as well as all 127
- üîç Investigate features ranked below 50 for removal

---

## üìä Example 2: Minimal Model Test

### Command
```
Dashboard ‚Üí Minimal Model Test ‚Üí Top 4 Features ‚Üí Run Test
```

### Console Output
```
üéØ Testing minimal model (top 4 features) vs full feature set...
üìä Calculating feature importance using xgboost...
‚úÖ Calculated importance for 127 features

üìä Full model (127 features): 0.5847 ¬± 0.0118
üìä Minimal model (4 features): 0.5812 ¬± 0.0124
üí° Use full model

üìä Minimal Model Test Results:
   Full model (127 features): 0.5847
   Minimal model (4 features): 0.5812
   Top features: home_elo_rating, away_elo_rating, home_rest_days, home_win_pct_last_10
   üí° Use full model
```

### Interpretation
- Full model scores **0.5847** (58.47% accuracy)
- Top 4 features score **0.5812** (58.12% accuracy)
- Difference: **0.0035** (0.35%)
- **Recommendation**: Use full model (extra features add value)

### Try This
Test with top 10 or top 20 features instead:
- If top 20 ‚âà full model: Simplify to 20 features
- If top 10 ‚âà full model: Simplify to 10 features

---

## üìä Example 3: Correlation Analysis

### Command
```
Dashboard ‚Üí Correlation Analysis ‚Üí Threshold: 0.80 ‚Üí Run Analysis
```

### Console Output
```
üîç Analyzing Feature Correlations...
‚úÖ Found 23 highly correlated feature pairs (>0.8)
‚ö†Ô∏è  Identified 12 potentially redundant features

üìä Correlation Analysis Results:
   High correlation pairs (>0.8): 23
   Potentially redundant features: 12

   Top 5 correlated pairs:
      ‚Ä¢ home_off_rating_last_5 ‚Üî home_off_rating_last_10: 0.967
      ‚Ä¢ away_def_rating_last_5 ‚Üî away_def_rating_last_10: 0.954
      ‚Ä¢ home_pace_last_5 ‚Üî home_pace_last_10: 0.923
      ‚Ä¢ home_win_pct_last_5 ‚Üî home_win_pct_last_10: 0.912
      ‚Ä¢ away_win_pct_last_5 ‚Üî away_win_pct_last_10: 0.908

üíæ Heatmap saved to: analysis_results/correlation_heatmap.png
```

### Interpretation
- **Last 5 vs Last 10** stats are highly correlated (0.91-0.97)
- These provide similar information
- **Recommendation**: Keep "last 10" versions, drop "last 5"
- Reduces features from 127 ‚Üí ~115 without losing information

### Action Items
- ‚úÖ Remove: `*_last_5` features (keep `*_last_10`)
- ‚úÖ Retrain model with reduced feature set
- ‚úÖ Compare performance (should be similar or better)

---

## üìä Example 4: Feature Validation

### Command
```
Dashboard ‚Üí Feature Validation ‚Üí Validate All Features
```

### Console Output
```
üß™ Validating Features...
‚úÖ Validated 30 features

üìä Feature Validation Results:
   Contributing features: 22
   Weak features: 5
   Noise features: 3

‚ö†Ô∏è FEATURES IDENTIFIED AS NOISE (consider removing):
  - home_travel_distance_last_3 (predictive power: 0.503)
  - away_home_games_last_5 (predictive power: 0.498)
  - home_back_to_back_games (predictive power: 0.512)
```

### Interpretation
- **22 features** contributing predictive power (>0.55)
- **3 features** are noise (<0.52 = no better than random)
- Travel distance and back-to-back features not helping

### Action Items
- ‚ùå Remove: `home_travel_distance_last_3`
- ‚ùå Remove: `away_home_games_last_5`
- ‚ö†Ô∏è Keep but monitor: `home_back_to_back_games` (borderline)

---

## üìä Example 5: Multi-Model Comparison

### Command
```
Dashboard ‚Üí Model Comparison ‚Üí Compare All Models
```

### Console Output (abbreviated)
```
üî¨ Comparing classification models...
  Testing XGBoost... ‚úÖ roc_auc: 0.5847
  Testing LightGBM... ‚úÖ roc_auc: 0.5812
  Testing Random Forest... ‚úÖ roc_auc: 0.5678
  Testing Gradient Boosting... ‚úÖ roc_auc: 0.5734
  Testing Logistic Regression... ‚úÖ roc_auc: 0.5489
  Testing Neural Network... ‚úÖ roc_auc: 0.5723
  Testing SVM (RBF)... ‚úÖ roc_auc: 0.5601
  Testing K-Nearest Neighbors... ‚úÖ roc_auc: 0.5423
  Testing Naive Bayes... ‚úÖ roc_auc: 0.5312
  Testing Decision Tree... ‚úÖ roc_auc: 0.5234
  Testing Bagging (RF)... ‚úÖ roc_auc: 0.5689

üèÜ Best Model: XGBoost (roc_auc: 0.5847)

Results:
   Model                      roc_auc_mean  roc_auc_std  accuracy_mean
0  XGBoost                    0.5847        0.0118       0.5623
1  LightGBM                   0.5812        0.0124       0.5589
2  Gradient Boosting          0.5734        0.0132       0.5512
3  Neural Network             0.5723        0.0145       0.5498
4  Bagging (RF)               0.5689        0.0128       0.5467
```

### Interpretation
- **XGBoost** is best performer (58.47% ROC-AUC)
- **LightGBM** close second (58.12%)
- **Traditional ML** much weaker (Logistic: 54.89%, Naive Bayes: 53.12%)
- Gradient boosting methods dominate

### Action Items
- ‚úÖ Continue using **XGBoost** (current choice validated)
- üîç Test **LightGBM** as alternative (similar performance, faster)
- üé≠ Test ensembles next (may improve beyond 58.47%)

---

## üìä Example 6: Ensemble Testing

### Command
```
Dashboard ‚Üí Ensemble Testing ‚Üí Test All Ensembles
```

### Console Output
```
üé≠ Testing All Ensemble Methods...

üó≥Ô∏è  Testing Voting Ensemble (classification)...
‚úÖ Voting Ensemble Score: 0.5871 ¬± 0.0112
üìä Improvement vs Best Single Model: +0.0024

üìö Testing Stacking Ensemble (classification)...
‚úÖ Stacking Ensemble Score: 0.5893 ¬± 0.0108
üìä Improvement vs Best Single Model: +0.0046

üèÜ Best Ensemble: STACKING (ROC-AUC: 0.5893)

üìä Ensemble Results:
   VOTING: 0.5871 (improvement: +0.0024)
   STACKING: 0.5893 (improvement: +0.0046)
```

### Interpretation
- **Stacking ensemble** improves by **0.46%** (58.93% vs 58.47%)
- **Voting ensemble** improves by **0.24%** (58.71% vs 58.47%)
- Small but meaningful improvements

### Decision Matrix
| Improvement | Recommendation |
|-------------|----------------|
| <0.01 (1%) | Not worth complexity |
| 0.01-0.02 (1-2%) | Consider for live betting |
| >0.02 (2%+) | Definitely use ensemble |

**In this case**: 0.46% improvement ‚Üí **Use stacking ensemble**

---

## üìä Example 7: Complete Analysis Report

### File: `analysis_results/feature_analysis_report.txt`

```
================================================================================
FEATURE ANALYSIS REPORT
================================================================================

1. CORRELATION ANALYSIS
--------------------------------------------------------------------------------
Threshold: 0.8
Highly correlated pairs: 23
Potentially redundant features: 12

Top 10 Highly Correlated Pairs:
  1. home_off_rating_last_5 <-> home_off_rating_last_10: 0.967
  2. away_def_rating_last_5 <-> away_def_rating_last_10: 0.954
  3. home_pace_last_5 <-> home_pace_last_10: 0.923
  4. home_win_pct_last_5 <-> home_win_pct_last_10: 0.912
  5. away_win_pct_last_5 <-> away_win_pct_last_10: 0.908
  6. home_ts_pct_last_5 <-> home_ts_pct_last_10: 0.894
  7. away_ts_pct_last_5 <-> away_ts_pct_last_10: 0.887
  8. home_efg_pct_last_5 <-> home_efg_pct_last_10: 0.876
  9. away_efg_pct_last_5 <-> away_efg_pct_last_10: 0.869
 10. home_def_rating_last_5 <-> home_def_rating_last_10: 0.851

2. FEATURE IMPORTANCE SCORES
--------------------------------------------------------------------------------

XGBOOST Method:
Top 15 Features:
  1. home_elo_rating: 0.124567
  2. away_elo_rating: 0.118432
  3. home_rest_days: 0.089234
  4. home_win_pct_last_10: 0.076891
  5. away_rest_days: 0.072456
  6. home_off_rating_last_5: 0.065234
  7. away_def_rating_last_5: 0.058923
  8. home_court_advantage: 0.051234
  9. home_pace_last_10: 0.047891
 10. away_win_pct_last_10: 0.044567
 11. home_ts_pct_last_10: 0.039234
 12. away_ts_pct_last_10: 0.036891
 13. home_efg_pct_last_10: 0.033456
 14. away_efg_pct_last_10: 0.031234
 15. home_off_rating_last_10: 0.028901

3. FEATURE VALIDATION RESULTS
--------------------------------------------------------------------------------
Contributing Features: 22
Weak Features: 5
Noise Features: 3

‚ö†Ô∏è FEATURES IDENTIFIED AS NOISE (consider removing):
  - home_travel_distance_last_3 (predictive power: 0.503)
  - away_home_games_last_5 (predictive power: 0.498)
  - home_back_to_back_games (predictive power: 0.512)

================================================================================
END OF REPORT
================================================================================
```

---

## üìä Example 8: Model Comparison CSV

### File: `analysis_results/model_comparison_results.csv`

| Model | roc_auc_mean | roc_auc_std | accuracy_mean | accuracy_std | precision_mean | recall_mean | f1_mean | fit_time_mean | task |
|-------|--------------|-------------|---------------|--------------|----------------|-------------|---------|---------------|------|
| XGBoost | 0.5847 | 0.0118 | 0.5623 | 0.0156 | 0.5589 | 0.5712 | 0.5649 | 2.34 | classification |
| LightGBM | 0.5812 | 0.0124 | 0.5589 | 0.0162 | 0.5556 | 0.5678 | 0.5615 | 1.87 | classification |
| Gradient Boosting | 0.5734 | 0.0132 | 0.5512 | 0.0171 | 0.5489 | 0.5601 | 0.5543 | 8.92 | classification |
| Neural Network | 0.5723 | 0.0145 | 0.5498 | 0.0183 | 0.5467 | 0.5589 | 0.5527 | 12.45 | classification |
| Bagging (RF) | 0.5689 | 0.0128 | 0.5467 | 0.0168 | 0.5445 | 0.5556 | 0.5499 | 15.67 | classification |
| Random Forest | 0.5678 | 0.0134 | 0.5456 | 0.0174 | 0.5434 | 0.5545 | 0.5488 | 5.23 | classification |

**Import to Excel for:**
- Sorting by ROC-AUC
- Creating comparison charts
- Tracking performance over time

---

## üéØ Summary of Key Findings

### From These Examples

**Feature Importance**:
- Top 2: ELO ratings (24% combined importance)
- Top 4: ELO + Rest days + Recent form (46% combined)
- Top 10: Cover 71% of total importance

**Feature Quality**:
- 12 redundant features (highly correlated pairs)
- 3 noise features (no predictive power)
- Potential reduction: 127 ‚Üí ~110 features

**Model Selection**:
- XGBoost best (58.47%)
- LightGBM close (58.12%)
- Stacking ensemble improves to 58.93%

**Recommendations**:
1. Remove `*_last_5` features (keep `*_last_10`)
2. Remove 3 noise features identified
3. Use stacking ensemble for live betting
4. Monitor performance quarterly

---

## üìä Expected ROI Impact

### Before Optimization
- Model: XGBoost (default params)
- Features: All 127 features
- Accuracy: 56.4%
- ROI: +2.8% long-term

### After Optimization
- Model: Stacking Ensemble (XGB + LGB + GB)
- Features: 110 features (removed redundant + noise)
- Accuracy: 58.9%
- **Expected ROI: +5.2% long-term** (+86% improvement)

**Key**: 2.5% accuracy improvement ‚Üí ~2.4% ROI improvement

---

**These examples show actual output formats you'll see when running the analysis tools!**
