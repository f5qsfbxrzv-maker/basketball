# QUICK COMMAND REFERENCE - Feature & Model Analysis

## ğŸ¯ Dashboard Access
```powershell
# Launch main dashboard
.\.venv\Scripts\python.exe main.py

# Navigate to: "ğŸ”¬ Feature & Model Analysis" tab
```

## âš¡ Quick Tests (2-5 minutes each)

### Feature Importance (Fast)
```python
python comprehensive_analysis.py features
```
**Output**: Top 10 features with importance scores

### Model Comparison (Fast)
```python
python comprehensive_analysis.py models
```
**Output**: Top 5 models with ROC-AUC scores

### Full Analysis Pipeline (30 minutes)
```python
python comprehensive_analysis.py full
```
**Output**: Complete reports in `analysis_results/`

## ğŸ” What Each Analysis Reveals

| Analysis | Time | Key Output | Action Item |
|----------|------|------------|-------------|
| **Correlation** | 2 min | Redundant features | Remove highly correlated pairs |
| **Validation** | 3 min | Noise features | Drop features with <0.52 predictive power |
| **Importance** | 2-5 min | Top contributors | Focus on top 10-20 features |
| **Minimal Test** | 3 min | Top 4 vs All | Simplify model if minimal = full |
| **Model Comparison** | 10 min | Best algorithm | Switch to best performer |
| **Ensemble Test** | 8 min | Voting/Stacking | Use ensemble if >1% improvement |

## ğŸ“Š Typical Workflow

### Phase 1: Feature Discovery (10 minutes)
1. Run **Correlation Analysis** â†’ Find redundant features
2. Run **Feature Validation** â†’ Find noise features
3. Create removal list

### Phase 2: Feature Selection (5 minutes)
1. Run **Feature Importance** (XGBoost) â†’ Rank features
2. Run **Minimal Model Test** â†’ Test top 4-10
3. Determine optimal feature set

### Phase 3: Model Selection (15 minutes)
1. Run **Multi-Model Comparison** â†’ Find best algorithm
2. Run **Ensemble Testing** â†’ Test if combining helps
3. Select final model architecture

### Phase 4: Implementation (5 minutes)
1. Update feature list in code
2. Retrain with optimal model
3. Backtest to validate improvements

## ğŸ¯ Common Questions & Quick Answers

### "Which features should I keep?"
```
Dashboard â†’ Analysis Tab â†’ Feature Importance (XGBoost)
Look for: Importance score >0.05
```

### "Can I use only 4 features instead of 100+?"
```
Dashboard â†’ Analysis Tab â†’ Minimal Model Test
If score difference <0.01: Yes, simplify!
```

### "Is XGBoost the best model?"
```
Dashboard â†’ Analysis Tab â†’ Multi-Model Comparison
Compare: XGBoost vs LightGBM vs Neural Net vs others
```

### "Should I use an ensemble?"
```
Dashboard â†’ Analysis Tab â†’ Ensemble Testing
If improvement >0.01 ROC-AUC: Yes
If <0.01: No, single model sufficient
```

### "Are my features creating noise?"
```
Dashboard â†’ Analysis Tab â†’ Feature Validation
Check: Predictive power column
Drop: Features with <0.52 power
```

### "Which features are redundant?"
```
Dashboard â†’ Analysis Tab â†’ Correlation Analysis
Check: Pairs with correlation >0.8
Keep only one from each pair
```

## ğŸ“ Results Locations

```
analysis_results/
â”œâ”€â”€ correlation_heatmap.png          # Visual correlation matrix
â”œâ”€â”€ feature_importance_xgboost.png   # XGBoost importance plot
â”œâ”€â”€ feature_importance_shap.png      # SHAP importance plot
â”œâ”€â”€ feature_analysis_report.txt      # Complete feature report
â”œâ”€â”€ model_comparison_report.txt      # Model performance report
â””â”€â”€ model_comparison_results.csv     # Detailed model scores
```

## ğŸš€ Optimization Pipeline

```
1. Download Data â†’ System Admin â†’ Download Historical Data
2. Feature Analysis â†’ Analysis Tab â†’ Run Complete Analysis
3. Review Reports â†’ analysis_results/ folder
4. Remove Noise â†’ Drop features with low importance/predictive power
5. Retrain Models â†’ System Admin â†’ Train ML Models
6. Validate â†’ System Admin â†’ Run Backtest
7. Deploy â†’ Use improved model for live betting
```

## âš¡ Time Estimates

| Task | Time Required |
|------|---------------|
| Correlation Analysis | 2 minutes |
| Feature Validation | 3 minutes |
| Feature Importance (XGBoost) | 2 minutes |
| Feature Importance (SHAP) | 10 minutes |
| Minimal Model Test | 3 minutes |
| Model Comparison (All) | 10 minutes |
| Ensemble Testing | 8 minutes |
| **Full Analysis Pipeline** | **20-30 minutes** |

## ğŸ“ Pro Tips

1. **Start with XGBoost importance** (fastest, good enough)
2. **Run SHAP only for final analysis** (slowest but most accurate)
3. **Test minimal model early** (may save hours of computation)
4. **Remove features iteratively** (analyze â†’ remove â†’ reanalyze)
5. **Compare ensembles last** (need baseline scores first)
6. **Save results before retraining** (backup for comparison)

## ğŸ”§ Troubleshooting Quick Fixes

| Error | Fix |
|-------|-----|
| "Training data not found" | Run Download Historical Data first |
| "Module not found" | Install: `pip install shap` |
| "Out of memory" | Reduce dataset or use XGBoost instead of SHAP |
| "Analysis taking too long" | Use quick tests, skip SHAP method |
| "Can't open results folder" | Check `analysis_results/` exists |

## ğŸ“ One-Liners

```python
# Get top 10 features (Python console)
from comprehensive_analysis import quick_feature_test
quick_feature_test(top_n=10)

# Compare top 5 models (Python console)
from comprehensive_analysis import quick_model_test
quick_model_test()

# Full analysis from Python
from comprehensive_analysis import run_complete_analysis
run_complete_analysis(target_column='home_wins')
```

## ğŸ¯ Decision Tree

```
Need to optimize model?
â”œâ”€ Yes â†’ Start here
â”‚   â”œâ”€ Improve accuracy?
â”‚   â”‚   â”œâ”€ Run Model Comparison â†’ Switch to best model
â”‚   â”‚   â””â”€ Run Ensemble Testing â†’ Use if improvement >1%
â”‚   â”‚
â”‚   â”œâ”€ Reduce overfitting?
â”‚   â”‚   â”œâ”€ Run Correlation â†’ Remove redundant features
â”‚   â”‚   â””â”€ Run Validation â†’ Remove noise features
â”‚   â”‚
â”‚   â””â”€ Speed up predictions?
â”‚       â””â”€ Run Minimal Model Test â†’ Use top 4-10 features
â”‚
â””â”€ No â†’ Skip analysis, use current model
```

---

**Version**: v1.0  
**Last Updated**: November 2025
